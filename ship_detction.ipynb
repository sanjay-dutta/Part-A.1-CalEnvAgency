{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b4523d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF: 2.10.1\n"
     ]
    }
   ],
   "source": [
    "# Setup & Paths\n",
    "import os, re, random, math, json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, average_precision_score\n",
    "\n",
    "print(\"TF:\", tf.__version__)\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n",
    "\n",
    "# data folders \n",
    "ROOT = Path(r\"C:/Users/sad77/Desktop/Part-A.1-CalEnvAgency/shipdata_2025\")\n",
    "CROPS_DIR  = ROOT / \"cropped_ship_dataset\"   \n",
    "SCENES_DIR = ROOT / \"scenes\"                 \n",
    "assert CROPS_DIR.exists(), f\"Missing: {CROPS_DIR}\"\n",
    "\n",
    "# Output folders \n",
    "OUT_ROOT = Path(\"outputs\")\n",
    "(OUT_ROOT / \"models\").mkdir(parents=True, exist_ok=True)\n",
    "(OUT_ROOT / \"figs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Training hyperparams \n",
    "IMG_SIZE   = 128      \n",
    "BATCH_SIZE = 64     \n",
    "EPOCHS     = 70\n",
    "VAL_SPLIT  = 0.20    \n",
    "\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "for g in tf.config.list_physical_devices(\"GPU\"):\n",
    "    try: tf.config.experimental.set_memory_growth(g, True)\n",
    "    except Exception: pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "887b2f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed: 4000  | bad filenames: 0\n",
      "label\n",
      "0    3000\n",
      "1    1000\n",
      "Name: count, dtype: int64\n",
      "Unique scenes: 434\n"
     ]
    }
   ],
   "source": [
    "import re, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "CROPS_DIR = Path(r\"C:/Users/sad77/Desktop/Part-A.1-CalEnvAgency/shipdata_2025/cropped_ship_dataset\")\n",
    "\n",
    "rows, bad = [], []\n",
    "for p in sorted([q for q in CROPS_DIR.rglob(\"*\") if q.suffix.lower() in {\".png\",\".jpg\",\".jpeg\"}]):\n",
    "    name = p.name\n",
    "    parts = name.rsplit(\".\", 1)[0].split(\"__\")\n",
    "    if len(parts) < 3:\n",
    "        bad.append(name); continue\n",
    "    raw_label, scene = parts[0].lower(), parts[1]\n",
    "    # label normalization\n",
    "    if raw_label in {\"0\",\"no\",\"noship\",\"negative\",\"neg\"}:\n",
    "        label = 0\n",
    "    elif raw_label in {\"1\",\"yes\",\"ship\",\"positive\",\"pos\"}:\n",
    "        label = 1\n",
    "    else:\n",
    "        # try first char digit\n",
    "        label = 1 if raw_label[:1] == \"1\" else (0 if raw_label[:1] == \"0\" else None)\n",
    "        if label is None:\n",
    "            bad.append(name); continue\n",
    "    rows.append((str(p), label, scene))\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"path\",\"label\",\"scene\"])\n",
    "print(\"Parsed:\", len(df), \" | bad filenames:\", len(bad))\n",
    "if bad: print(\"Bad examples:\", bad[:20])\n",
    "print(df[\"label\"].value_counts(dropna=False))\n",
    "print(\"Unique scenes:\", df[\"scene\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10886c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n",
      "train    3148\n",
      "val       852\n",
      "Name: count, dtype: int64\n",
      "label_name  No-Ship  Ship\n",
      "split                    \n",
      "train          2358   790\n",
      "val             642   210\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "scenes = df.groupby(\"scene\")[\"label\"].agg([\"sum\",\"count\"]).reset_index()\n",
    "scenes[\"has_pos\"] = scenes[\"sum\"] > 0\n",
    "\n",
    "\n",
    "pos_scenes = scenes[scenes[\"has_pos\"]][\"scene\"].tolist()\n",
    "neg_scenes = scenes[~scenes[\"has_pos\"]][\"scene\"].tolist()\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(pos_scenes); rng.shuffle(neg_scenes)\n",
    "\n",
    "n_val = max(1, int(len(scenes)*VAL_SPLIT))\n",
    "val_sel = []\n",
    "if pos_scenes:\n",
    "    val_sel.append(pos_scenes.pop())  \n",
    "while len(val_sel) < n_val and (pos_scenes or neg_scenes):\n",
    "    pick_from = pos_scenes if (len(pos_scenes) > 0 and len(val_sel) < n_val*0.5) else neg_scenes\n",
    "    if not pick_from: pick_from = pos_scenes or neg_scenes\n",
    "    val_sel.append(pick_from.pop())\n",
    "\n",
    "val_scenes = set(val_sel)\n",
    "df[\"split\"] = np.where(df[\"scene\"].isin(val_scenes), \"val\", \"train\")\n",
    "df[\"label_name\"] = df[\"label\"].map({0:\"No-Ship\", 1:\"Ship\"})\n",
    "\n",
    "print(df[\"split\"].value_counts())\n",
    "print(df.groupby([\"split\",\"label_name\"]).size().unstack(fill_value=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dda8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights used: {0: 1.0, 1: 1.4}\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def decode_resize(path, label):\n",
    "    img = tf.io.decode_png(tf.io.read_file(path), channels=3)\n",
    "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE), antialias=True)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img, tf.cast(label, tf.int32)\n",
    "\n",
    "def augment(img, label):\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_flip_up_down(img)\n",
    "    k   = tf.random.uniform([], 0, 4, dtype=tf.int32)  \n",
    "    img = tf.image.rot90(img, k)\n",
    "    img = tf.image.random_brightness(img, 0.05)\n",
    "    img = tf.image.random_contrast(img, 0.95, 1.05)\n",
    "    return img, label\n",
    "\n",
    "def make_ds(frame, training=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((frame[\"path\"].values, frame[\"label\"].values))\n",
    "    if training:\n",
    "        ds = ds.shuffle(8000, seed=SEED)\n",
    "    ds = ds.map(decode_resize, num_parallel_calls=AUTOTUNE)\n",
    "    if training:\n",
    "        ds = ds.map(augment, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_df = df[df[\"split\"]==\"train\"].reset_index(drop=True)\n",
    "val_df   = df[df[\"split\"]==\"val\"].reset_index(drop=True)\n",
    "train_ds = make_ds(train_df, training=True)\n",
    "val_ds   = make_ds(val_df,   training=False)\n",
    "\n",
    "pos = int((train_df[\"label\"]==1).sum()); neg = int((train_df[\"label\"]==0).sum())\n",
    "total = len(train_df)\n",
    "w_pos = total / (2*max(1,pos))\n",
    "w_neg = total / (2*max(1,neg))\n",
    "class_weights = {0: 1.0, 1: float(min(w_pos, 1.4))}\n",
    "print(\"Class weights used:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e28e100b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vit_binary\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 128, 128, 32  896         ['input_5[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 8, 8, 256)    2097408     ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)            (None, 64, 256)      0           ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " add_class_token_and_pos_4 (Add  (None, 65, 256)     16896       ['reshape_4[0][0]']              \n",
      " ClassTokenAndPos)                                                                                \n",
      "                                                                                                  \n",
      " layer_normalization_52 (LayerN  (None, 65, 256)     512         ['add_class_token_and_pos_4[0][0]\n",
      " ormalization)                                                   ']                               \n",
      "                                                                                                  \n",
      " multi_head_attention_24 (Multi  (None, 65, 256)     1051904     ['layer_normalization_52[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_76 (Dropout)           (None, 65, 256)      0           ['multi_head_attention_24[0][0]']\n",
      "                                                                                                  \n",
      " add_48 (Add)                   (None, 65, 256)      0           ['add_class_token_and_pos_4[0][0]\n",
      "                                                                 ',                               \n",
      "                                                                  'dropout_76[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_53 (LayerN  (None, 65, 256)     512         ['add_48[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_52 (Dense)               (None, 65, 512)      131584      ['layer_normalization_53[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_77 (Dropout)           (None, 65, 512)      0           ['dense_52[0][0]']               \n",
      "                                                                                                  \n",
      " dense_53 (Dense)               (None, 65, 256)      131328      ['dropout_77[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_78 (Dropout)           (None, 65, 256)      0           ['dense_53[0][0]']               \n",
      "                                                                                                  \n",
      " add_49 (Add)                   (None, 65, 256)      0           ['add_48[0][0]',                 \n",
      "                                                                  'dropout_78[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_54 (LayerN  (None, 65, 256)     512         ['add_49[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_25 (Multi  (None, 65, 256)     1051904     ['layer_normalization_54[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_79 (Dropout)           (None, 65, 256)      0           ['multi_head_attention_25[0][0]']\n",
      "                                                                                                  \n",
      " add_50 (Add)                   (None, 65, 256)      0           ['add_49[0][0]',                 \n",
      "                                                                  'dropout_79[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_55 (LayerN  (None, 65, 256)     512         ['add_50[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_54 (Dense)               (None, 65, 512)      131584      ['layer_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_80 (Dropout)           (None, 65, 512)      0           ['dense_54[0][0]']               \n",
      "                                                                                                  \n",
      " dense_55 (Dense)               (None, 65, 256)      131328      ['dropout_80[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_81 (Dropout)           (None, 65, 256)      0           ['dense_55[0][0]']               \n",
      "                                                                                                  \n",
      " add_51 (Add)                   (None, 65, 256)      0           ['add_50[0][0]',                 \n",
      "                                                                  'dropout_81[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_56 (LayerN  (None, 65, 256)     512         ['add_51[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_26 (Multi  (None, 65, 256)     1051904     ['layer_normalization_56[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_82 (Dropout)           (None, 65, 256)      0           ['multi_head_attention_26[0][0]']\n",
      "                                                                                                  \n",
      " add_52 (Add)                   (None, 65, 256)      0           ['add_51[0][0]',                 \n",
      "                                                                  'dropout_82[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_57 (LayerN  (None, 65, 256)     512         ['add_52[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_56 (Dense)               (None, 65, 512)      131584      ['layer_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_83 (Dropout)           (None, 65, 512)      0           ['dense_56[0][0]']               \n",
      "                                                                                                  \n",
      " dense_57 (Dense)               (None, 65, 256)      131328      ['dropout_83[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_84 (Dropout)           (None, 65, 256)      0           ['dense_57[0][0]']               \n",
      "                                                                                                  \n",
      " add_53 (Add)                   (None, 65, 256)      0           ['add_52[0][0]',                 \n",
      "                                                                  'dropout_84[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_58 (LayerN  (None, 65, 256)     512         ['add_53[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_27 (Multi  (None, 65, 256)     1051904     ['layer_normalization_58[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_58[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_85 (Dropout)           (None, 65, 256)      0           ['multi_head_attention_27[0][0]']\n",
      "                                                                                                  \n",
      " add_54 (Add)                   (None, 65, 256)      0           ['add_53[0][0]',                 \n",
      "                                                                  'dropout_85[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_59 (LayerN  (None, 65, 256)     512         ['add_54[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_58 (Dense)               (None, 65, 512)      131584      ['layer_normalization_59[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_86 (Dropout)           (None, 65, 512)      0           ['dense_58[0][0]']               \n",
      "                                                                                                  \n",
      " dense_59 (Dense)               (None, 65, 256)      131328      ['dropout_86[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_87 (Dropout)           (None, 65, 256)      0           ['dense_59[0][0]']               \n",
      "                                                                                                  \n",
      " add_55 (Add)                   (None, 65, 256)      0           ['add_54[0][0]',                 \n",
      "                                                                  'dropout_87[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_60 (LayerN  (None, 65, 256)     512         ['add_55[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_28 (Multi  (None, 65, 256)     1051904     ['layer_normalization_60[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_88 (Dropout)           (None, 65, 256)      0           ['multi_head_attention_28[0][0]']\n",
      "                                                                                                  \n",
      " add_56 (Add)                   (None, 65, 256)      0           ['add_55[0][0]',                 \n",
      "                                                                  'dropout_88[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_61 (LayerN  (None, 65, 256)     512         ['add_56[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_60 (Dense)               (None, 65, 512)      131584      ['layer_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_89 (Dropout)           (None, 65, 512)      0           ['dense_60[0][0]']               \n",
      "                                                                                                  \n",
      " dense_61 (Dense)               (None, 65, 256)      131328      ['dropout_89[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_90 (Dropout)           (None, 65, 256)      0           ['dense_61[0][0]']               \n",
      "                                                                                                  \n",
      " add_57 (Add)                   (None, 65, 256)      0           ['add_56[0][0]',                 \n",
      "                                                                  'dropout_90[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_62 (LayerN  (None, 65, 256)     512         ['add_57[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_29 (Multi  (None, 65, 256)     1051904     ['layer_normalization_62[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_91 (Dropout)           (None, 65, 256)      0           ['multi_head_attention_29[0][0]']\n",
      "                                                                                                  \n",
      " add_58 (Add)                   (None, 65, 256)      0           ['add_57[0][0]',                 \n",
      "                                                                  'dropout_91[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_63 (LayerN  (None, 65, 256)     512         ['add_58[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_62 (Dense)               (None, 65, 512)      131584      ['layer_normalization_63[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_92 (Dropout)           (None, 65, 512)      0           ['dense_62[0][0]']               \n",
      "                                                                                                  \n",
      " dense_63 (Dense)               (None, 65, 256)      131328      ['dropout_92[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_93 (Dropout)           (None, 65, 256)      0           ['dense_63[0][0]']               \n",
      "                                                                                                  \n",
      " add_59 (Add)                   (None, 65, 256)      0           ['add_58[0][0]',                 \n",
      "                                                                  'dropout_93[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_64 (LayerN  (None, 65, 256)     512         ['add_59[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_4 (Sl  (None, 256)         0           ['layer_normalization_64[0][0]'] \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dropout_94 (Dropout)           (None, 256)          0           ['tf.__operators__.getitem_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_64 (Dense)               (None, 1)            257         ['dropout_94[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,011,009\n",
      "Trainable params: 10,011,009\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build ViT \n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "class AddClassTokenAndPos(layers.Layer):\n",
    "    def __init__(self, num_patches, dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.dim = dim\n",
    "        self.cls_token = self.add_weight(\n",
    "            \"cls_token\", shape=(1, 1, dim), initializer=\"zeros\", trainable=True\n",
    "        )\n",
    "        self.pos_emb = self.add_weight(\n",
    "            \"pos_emb\", shape=(1, num_patches + 1, dim), initializer=\"zeros\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, tokens):\n",
    "        B = tf.shape(tokens)[0]\n",
    "        cls = tf.repeat(self.cls_token, repeats=B, axis=0)  \n",
    "        x = tf.concat([cls, tokens], axis=1)                \n",
    "        return x + self.pos_emb\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"num_patches\": self.num_patches, \"dim\": self.dim, **super().get_config()}\n",
    "\n",
    "def build_vit(\n",
    "    img_size=128, patch=16, dim=256, depth=6, heads=4, mlp_dim=512, drop=0.1, l2=1e-6\n",
    ") -> tf.keras.Model:\n",
    "    assert img_size % patch == 0, \"img_size must be divisible by patch\"\n",
    "    n_h = img_size // patch\n",
    "    n_w = img_size // patch\n",
    "    num_patches = n_h * n_w\n",
    "\n",
    "    inputs = layers.Input((img_size, img_size, 3))\n",
    "    x = layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(inputs)\n",
    "\n",
    "    \n",
    "    # Output: [B, n_h, n_w, dim]\n",
    "    x = layers.Conv2D(dim, kernel_size=patch, strides=patch, padding=\"valid\")(x)\n",
    "    # Flatten to tokens: [B, N, dim]\n",
    "    x = layers.Reshape((num_patches, dim))(x)\n",
    "\n",
    "    # Add [CLS] and positional embedding\n",
    "    x = AddClassTokenAndPos(num_patches=num_patches, dim=dim)(x)\n",
    "\n",
    "    # Transformer encoder blocks\n",
    "    for _ in range(depth):\n",
    "        # Norm → MHA → skip\n",
    "        h = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        h = layers.MultiHeadAttention(num_heads=heads, key_dim=dim, dropout=drop)(h, h)\n",
    "        x = layers.Add()([x, layers.Dropout(drop)(h)])\n",
    "        # Norm → MLP → skip\n",
    "        h = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        h = layers.Dense(mlp_dim, activation=\"gelu\")(h)\n",
    "        h = layers.Dropout(drop)(h)\n",
    "        h = layers.Dense(dim)(h)\n",
    "        x = layers.Add()([x, layers.Dropout(drop)(h)])\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    cls = x[:, 0]  # [CLS]\n",
    "    cls = layers.Dropout(drop)(cls)\n",
    "\n",
    "    logits = layers.Dense(\n",
    "        1,\n",
    "        kernel_regularizer=regularizers.l2(l2),\n",
    "        bias_regularizer=regularizers.l2(l2)\n",
    "    )(cls)  \n",
    "\n",
    "    return models.Model(inputs, logits, name=\"vit_binary\")\n",
    "\n",
    "vit = build_vit(\n",
    "    img_size=IMG_SIZE, patch=16, dim=256, depth=6, heads=4, mlp_dim=512, drop=0.1, l2=1e-6\n",
    ")\n",
    "vit.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799a79fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT class weights: {0: 1.0, 1: 1.4}\n",
      "Epoch 1/70\n",
      "50/50 - 32s - loss: 0.8954 - acc: 0.6630 - auc: 0.5042 - val_loss: 0.5930 - val_acc: 0.7535 - val_auc: 0.5208 - 32s/epoch - 635ms/step\n",
      "Epoch 2/70\n",
      "50/50 - 5s - loss: 0.7290 - acc: 0.7084 - auc: 0.5068 - val_loss: 0.6285 - val_acc: 0.7535 - val_auc: 0.5193 - 5s/epoch - 109ms/step\n",
      "Epoch 3/70\n",
      "50/50 - 6s - loss: 0.7155 - acc: 0.7182 - auc: 0.5315 - val_loss: 0.6057 - val_acc: 0.7477 - val_auc: 0.5229 - 6s/epoch - 111ms/step\n",
      "Epoch 4/70\n",
      "50/50 - 6s - loss: 0.6996 - acc: 0.7141 - auc: 0.5710 - val_loss: 0.6375 - val_acc: 0.6714 - val_auc: 0.5448 - 6s/epoch - 112ms/step\n",
      "Epoch 5/70\n",
      "50/50 - 5s - loss: 0.6799 - acc: 0.7141 - auc: 0.6207 - val_loss: 0.5976 - val_acc: 0.6984 - val_auc: 0.5986 - 5s/epoch - 109ms/step\n",
      "Epoch 6/70\n",
      "50/50 - 5s - loss: 0.7034 - acc: 0.6874 - auc: 0.5968 - val_loss: 0.8008 - val_acc: 0.7535 - val_auc: 0.5838 - 5s/epoch - 107ms/step\n",
      "Epoch 7/70\n",
      "50/50 - 5s - loss: 0.6868 - acc: 0.7392 - auc: 0.5856 - val_loss: 0.5749 - val_acc: 0.7535 - val_auc: 0.5758 - 5s/epoch - 110ms/step\n",
      "Epoch 8/70\n",
      "50/50 - 5s - loss: 0.6683 - acc: 0.7103 - auc: 0.6536 - val_loss: 0.5699 - val_acc: 0.7535 - val_auc: 0.6099 - 5s/epoch - 110ms/step\n",
      "Epoch 9/70\n",
      "50/50 - 5s - loss: 0.6891 - acc: 0.7230 - auc: 0.5950 - val_loss: 0.5412 - val_acc: 0.7535 - val_auc: 0.6505 - 5s/epoch - 109ms/step\n",
      "Epoch 10/70\n",
      "50/50 - 6s - loss: 0.5890 - acc: 0.7348 - auc: 0.7623 - val_loss: 0.4800 - val_acc: 0.8087 - val_auc: 0.8019 - 6s/epoch - 112ms/step\n",
      "Epoch 11/70\n",
      "50/50 - 6s - loss: 0.4786 - acc: 0.8167 - auc: 0.8703 - val_loss: 0.4316 - val_acc: 0.8322 - val_auc: 0.8136 - 6s/epoch - 110ms/step\n",
      "Epoch 12/70\n",
      "50/50 - 6s - loss: 0.5040 - acc: 0.8316 - auc: 0.8576 - val_loss: 0.3517 - val_acc: 0.8721 - val_auc: 0.9059 - 6s/epoch - 111ms/step\n",
      "Epoch 13/70\n",
      "50/50 - 5s - loss: 0.4385 - acc: 0.8466 - auc: 0.8975 - val_loss: 0.3709 - val_acc: 0.8638 - val_auc: 0.9096 - 5s/epoch - 106ms/step\n",
      "Epoch 14/70\n",
      "50/50 - 5s - loss: 0.3299 - acc: 0.9069 - auc: 0.9506 - val_loss: 0.3528 - val_acc: 0.8756 - val_auc: 0.9528 - 5s/epoch - 107ms/step\n",
      "Epoch 15/70\n",
      "50/50 - 6s - loss: 0.3138 - acc: 0.9114 - auc: 0.9568 - val_loss: 0.2275 - val_acc: 0.9331 - val_auc: 0.9711 - 6s/epoch - 110ms/step\n",
      "Epoch 16/70\n",
      "50/50 - 6s - loss: 0.2166 - acc: 0.9495 - auc: 0.9848 - val_loss: 0.1963 - val_acc: 0.9542 - val_auc: 0.9815 - 6s/epoch - 112ms/step\n",
      "Epoch 17/70\n",
      "50/50 - 6s - loss: 0.1919 - acc: 0.9625 - auc: 0.9892 - val_loss: 0.1914 - val_acc: 0.9601 - val_auc: 0.9775 - 6s/epoch - 111ms/step\n",
      "Epoch 18/70\n",
      "50/50 - 5s - loss: 0.1853 - acc: 0.9657 - auc: 0.9906 - val_loss: 0.2277 - val_acc: 0.9308 - val_auc: 0.9790 - 5s/epoch - 107ms/step\n",
      "Epoch 19/70\n",
      "50/50 - 5s - loss: 0.1756 - acc: 0.9654 - auc: 0.9914 - val_loss: 0.1671 - val_acc: 0.9648 - val_auc: 0.9897 - 5s/epoch - 110ms/step\n",
      "Epoch 20/70\n",
      "50/50 - 5s - loss: 0.1781 - acc: 0.9666 - auc: 0.9911 - val_loss: 0.1838 - val_acc: 0.9554 - val_auc: 0.9908 - 5s/epoch - 106ms/step\n",
      "Epoch 21/70\n",
      "50/50 - 5s - loss: 0.1834 - acc: 0.9647 - auc: 0.9892 - val_loss: 0.1906 - val_acc: 0.9566 - val_auc: 0.9758 - 5s/epoch - 106ms/step\n",
      "Epoch 22/70\n",
      "50/50 - 5s - loss: 0.1545 - acc: 0.9746 - auc: 0.9953 - val_loss: 0.1551 - val_acc: 0.9683 - val_auc: 0.9941 - 5s/epoch - 110ms/step\n",
      "Epoch 23/70\n",
      "50/50 - 5s - loss: 0.1583 - acc: 0.9755 - auc: 0.9938 - val_loss: 0.1560 - val_acc: 0.9754 - val_auc: 0.9879 - 5s/epoch - 106ms/step\n",
      "Epoch 24/70\n",
      "50/50 - 5s - loss: 0.1564 - acc: 0.9768 - auc: 0.9934 - val_loss: 0.1818 - val_acc: 0.9566 - val_auc: 0.9900 - 5s/epoch - 106ms/step\n",
      "Epoch 25/70\n",
      "50/50 - 5s - loss: 0.1600 - acc: 0.9746 - auc: 0.9944 - val_loss: 0.1608 - val_acc: 0.9742 - val_auc: 0.9920 - 5s/epoch - 106ms/step\n",
      "Epoch 26/70\n",
      "50/50 - 5s - loss: 0.1551 - acc: 0.9759 - auc: 0.9948 - val_loss: 0.1498 - val_acc: 0.9671 - val_auc: 0.9934 - 5s/epoch - 109ms/step\n",
      "Epoch 27/70\n",
      "50/50 - 5s - loss: 0.1511 - acc: 0.9790 - auc: 0.9943 - val_loss: 0.2939 - val_acc: 0.9073 - val_auc: 0.9782 - 5s/epoch - 108ms/step\n",
      "Epoch 28/70\n",
      "50/50 - 5s - loss: 0.1560 - acc: 0.9740 - auc: 0.9952 - val_loss: 0.1572 - val_acc: 0.9683 - val_auc: 0.9952 - 5s/epoch - 106ms/step\n",
      "Epoch 29/70\n",
      "50/50 - 5s - loss: 0.1720 - acc: 0.9673 - auc: 0.9927 - val_loss: 0.1678 - val_acc: 0.9613 - val_auc: 0.9951 - 5s/epoch - 107ms/step\n",
      "Epoch 30/70\n",
      "50/50 - 5s - loss: 0.1561 - acc: 0.9743 - auc: 0.9942 - val_loss: 0.1734 - val_acc: 0.9566 - val_auc: 0.9864 - 5s/epoch - 106ms/step\n",
      "Epoch 31/70\n",
      "50/50 - 5s - loss: 0.1429 - acc: 0.9806 - auc: 0.9950 - val_loss: 0.1484 - val_acc: 0.9683 - val_auc: 0.9932 - 5s/epoch - 110ms/step\n",
      "Epoch 32/70\n",
      "50/50 - 5s - loss: 0.1482 - acc: 0.9778 - auc: 0.9957 - val_loss: 0.1326 - val_acc: 0.9742 - val_auc: 0.9947 - 5s/epoch - 109ms/step\n",
      "Epoch 33/70\n",
      "50/50 - 5s - loss: 0.1484 - acc: 0.9774 - auc: 0.9953 - val_loss: 0.1284 - val_acc: 0.9789 - val_auc: 0.9970 - 5s/epoch - 109ms/step\n",
      "Epoch 34/70\n",
      "50/50 - 5s - loss: 0.1524 - acc: 0.9755 - auc: 0.9955 - val_loss: 0.2014 - val_acc: 0.9472 - val_auc: 0.9908 - 5s/epoch - 106ms/step\n",
      "Epoch 35/70\n",
      "50/50 - 5s - loss: 0.1542 - acc: 0.9755 - auc: 0.9954 - val_loss: 0.1506 - val_acc: 0.9683 - val_auc: 0.9963 - 5s/epoch - 106ms/step\n",
      "Epoch 36/70\n",
      "50/50 - 5s - loss: 0.1471 - acc: 0.9790 - auc: 0.9956 - val_loss: 0.1803 - val_acc: 0.9577 - val_auc: 0.9879 - 5s/epoch - 106ms/step\n",
      "Epoch 37/70\n",
      "50/50 - 5s - loss: 0.1509 - acc: 0.9752 - auc: 0.9955 - val_loss: 0.1542 - val_acc: 0.9648 - val_auc: 0.9971 - 5s/epoch - 107ms/step\n",
      "Epoch 38/70\n",
      "50/50 - 5s - loss: 0.1410 - acc: 0.9803 - auc: 0.9963 - val_loss: 0.1283 - val_acc: 0.9824 - val_auc: 0.9965 - 5s/epoch - 110ms/step\n",
      "Epoch 39/70\n",
      "50/50 - 5s - loss: 0.1456 - acc: 0.9781 - auc: 0.9950 - val_loss: 0.1347 - val_acc: 0.9765 - val_auc: 0.9940 - 5s/epoch - 107ms/step\n",
      "Epoch 40/70\n",
      "50/50 - 6s - loss: 0.1327 - acc: 0.9844 - auc: 0.9963 - val_loss: 0.1282 - val_acc: 0.9765 - val_auc: 0.9962 - 6s/epoch - 112ms/step\n",
      "Epoch 41/70\n",
      "50/50 - 5s - loss: 0.1293 - acc: 0.9832 - auc: 0.9978 - val_loss: 0.1276 - val_acc: 0.9812 - val_auc: 0.9973 - 5s/epoch - 110ms/step\n",
      "Epoch 42/70\n",
      "50/50 - 5s - loss: 0.1337 - acc: 0.9828 - auc: 0.9963 - val_loss: 0.1382 - val_acc: 0.9754 - val_auc: 0.9811 - 5s/epoch - 106ms/step\n",
      "Epoch 43/70\n",
      "50/50 - 5s - loss: 0.1304 - acc: 0.9844 - auc: 0.9978 - val_loss: 0.1926 - val_acc: 0.9589 - val_auc: 0.9794 - 5s/epoch - 107ms/step\n",
      "Epoch 44/70\n",
      "50/50 - 5s - loss: 0.1469 - acc: 0.9784 - auc: 0.9952 - val_loss: 0.1673 - val_acc: 0.9648 - val_auc: 0.9933 - 5s/epoch - 106ms/step\n",
      "Epoch 45/70\n",
      "50/50 - 5s - loss: 0.1503 - acc: 0.9787 - auc: 0.9940 - val_loss: 0.1328 - val_acc: 0.9765 - val_auc: 0.9966 - 5s/epoch - 105ms/step\n",
      "Epoch 46/70\n",
      "50/50 - 5s - loss: 0.1298 - acc: 0.9838 - auc: 0.9979 - val_loss: 0.1882 - val_acc: 0.9519 - val_auc: 0.9910 - 5s/epoch - 108ms/step\n",
      "Epoch 47/70\n",
      "50/50 - 6s - loss: 0.1360 - acc: 0.9803 - auc: 0.9971 - val_loss: 0.1194 - val_acc: 0.9859 - val_auc: 0.9977 - 6s/epoch - 110ms/step\n",
      "Epoch 48/70\n",
      "50/50 - 5s - loss: 0.1480 - acc: 0.9762 - auc: 0.9960 - val_loss: 0.1240 - val_acc: 0.9836 - val_auc: 0.9894 - 5s/epoch - 109ms/step\n",
      "Epoch 49/70\n",
      "50/50 - 5s - loss: 0.1360 - acc: 0.9816 - auc: 0.9965 - val_loss: 0.1499 - val_acc: 0.9695 - val_auc: 0.9959 - 5s/epoch - 107ms/step\n",
      "Epoch 50/70\n",
      "50/50 - 5s - loss: 0.1337 - acc: 0.9835 - auc: 0.9964 - val_loss: 0.1246 - val_acc: 0.9812 - val_auc: 0.9977 - 5s/epoch - 107ms/step\n",
      "Epoch 51/70\n",
      "50/50 - 5s - loss: 0.1279 - acc: 0.9848 - auc: 0.9973 - val_loss: 0.1210 - val_acc: 0.9824 - val_auc: 0.9979 - 5s/epoch - 108ms/step\n",
      "Epoch 52/70\n",
      "50/50 - 5s - loss: 0.1248 - acc: 0.9873 - auc: 0.9968 - val_loss: 0.1407 - val_acc: 0.9730 - val_auc: 0.9943 - 5s/epoch - 106ms/step\n",
      "Epoch 53/70\n",
      "50/50 - 5s - loss: 0.1275 - acc: 0.9851 - auc: 0.9980 - val_loss: 0.1650 - val_acc: 0.9671 - val_auc: 0.9810 - 5s/epoch - 107ms/step\n",
      "Epoch 54/70\n",
      "50/50 - 5s - loss: 0.1302 - acc: 0.9835 - auc: 0.9976 - val_loss: 0.1306 - val_acc: 0.9777 - val_auc: 0.9976 - 5s/epoch - 107ms/step\n",
      "Epoch 55/70\n",
      "50/50 - 5s - loss: 0.1292 - acc: 0.9841 - auc: 0.9979 - val_loss: 0.1221 - val_acc: 0.9812 - val_auc: 0.9978 - 5s/epoch - 106ms/step\n",
      "Epoch 56/70\n",
      "50/50 - 5s - loss: 0.1242 - acc: 0.9873 - auc: 0.9979 - val_loss: 0.1388 - val_acc: 0.9683 - val_auc: 0.9936 - 5s/epoch - 107ms/step\n",
      "Epoch 57/70\n",
      "50/50 - 5s - loss: 0.1912 - acc: 0.9609 - auc: 0.9876 - val_loss: 0.1804 - val_acc: 0.9542 - val_auc: 0.9874 - 5s/epoch - 109ms/step\n",
      "Training done. Saved best weights → outputs\\models\\vit_final.weights.h5\n"
     ]
    }
   ],
   "source": [
    "# Train ViT\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, AUC\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "vit.compile(\n",
    "    optimizer=optimizers.Adam(2e-4),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing=0.03),\n",
    "    metrics=[BinaryAccuracy(threshold=0.0, name=\"acc\"), AUC(from_logits=True, name=\"auc\")]\n",
    ")\n",
    "\n",
    "cb = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\",\n",
    "                                     patience=10, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        OUT_ROOT/\"models/vit_final.weights.h5\",\n",
    "        save_best_only=True, save_weights_only=True,\n",
    "        monitor=\"val_loss\", mode=\"min\"\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"ViT class weights:\", class_weights)\n",
    "_ = vit.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=cb,\n",
    "    verbose=2\n",
    ")\n",
    "print(\"Training done. Saved best weights →\", OUT_ROOT/\"models/vit_final.weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8d5177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP=0.992 | Best-F1=0.972 @ thr=0.40\n",
      "CM @ best-F1 [TN FP FN TP]: (635, 7, 5, 205)\n",
      "t=0.10  P=0.908  R=0.990  F1=0.948  TN=621 FP=21 FN=2 TP=208\n",
      "t=0.15  P=0.920  R=0.981  F1=0.949  TN=624 FP=18 FN=4 TP=206\n",
      "t=0.20  P=0.928  R=0.981  F1=0.954  TN=626 FP=16 FN=4 TP=206\n",
      "t=0.25  P=0.945  R=0.981  F1=0.963  TN=630 FP=12 FN=4 TP=206\n",
      "t=0.30  P=0.953  R=0.976  F1=0.965  TN=632 FP=10 FN=5 TP=205\n",
      "t=0.40  P=0.967  R=0.976  F1=0.972  TN=635 FP=7 FN=5 TP=205\n",
      "t=0.50  P=0.971  R=0.971  F1=0.971  TN=636 FP=6 FN=6 TP=204\n",
      "Saved → figs: vit_confusion.png, vit_pr.png | metrics: vit_val_metrics.json\n"
     ]
    }
   ],
   "source": [
    "# Evaluation \n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Rebuild & load weights \n",
    "eval_model = build_vit(\n",
    "    img_size=IMG_SIZE, patch=16, dim=256, depth=6, heads=4, mlp_dim=512, drop=0.1, l2=1e-6\n",
    ")\n",
    "eval_model.load_weights(OUT_ROOT/\"models/vit_final.weights.h5\")\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "def decode_resize_eval(path, label):\n",
    "    img = tf.io.decode_png(tf.io.read_file(path), channels=3)\n",
    "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE), antialias=True)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img, tf.cast(label, tf.int32)\n",
    "\n",
    "val_df_eval = df[df[\"split\"]==\"val\"].reset_index(drop=True)\n",
    "val_ds_eval = (tf.data.Dataset\n",
    "               .from_tensor_slices((val_df_eval[\"path\"].values, val_df_eval[\"label\"].values))\n",
    "               .map(decode_resize_eval, num_parallel_calls=AUTOTUNE)\n",
    "               .batch(64).prefetch(AUTOTUNE))\n",
    "\n",
    "ys, yh = [], []\n",
    "for x,y in val_ds_eval:\n",
    "    p = eval_model.predict(x, verbose=0)\n",
    "    ys.append(y.numpy()); yh.append(p)\n",
    "y_true = np.concatenate(ys).astype(int)\n",
    "logits = np.concatenate(yh).ravel().astype(np.float32)\n",
    "probs  = 1/(1+np.exp(-logits))\n",
    "probs  = np.clip(probs, 1e-7, 1-1e-7)\n",
    "\n",
    "\n",
    "ap = average_precision_score(y_true, probs)\n",
    "prec, rec, thr = precision_recall_curve(y_true, probs)\n",
    "\n",
    "ts = np.linspace(0.05, 0.50, 46) \n",
    "best_f1, best_t, best_cm = -1.0, 0.5, None\n",
    "for t in ts:\n",
    "    yhat = (probs >= t).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, yhat, labels=[0,1]).ravel()\n",
    "    P = tp/(tp+fp+1e-9); R = tp/(tp+fn+1e-9)\n",
    "    F1 = 2*P*R/(P+R+1e-9)\n",
    "    if F1 > best_f1:\n",
    "        best_f1, best_t, best_cm = F1, t, (tn,fp,fn,tp)\n",
    "\n",
    "print(f\"AP={ap:.3f} | Best-F1={best_f1:.3f} @ thr={best_t:.2f}\")\n",
    "print(\"CM @ best-F1 [TN FP FN TP]:\", best_cm)\n",
    "\n",
    "\n",
    "for t in [0.10, 0.15, 0.20, 0.25, 0.30, 0.40, 0.50]:\n",
    "    yhat = (probs >= t).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, yhat, labels=[0,1]).ravel()\n",
    "    P = tp/(tp+fp+1e-9); R = tp/(tp+fn+1e-9); F1 = 2*P*R/(P+R+1e-9)\n",
    "    print(f\"t={t:.2f}  P={P:.3f}  R={R:.3f}  F1={F1:.3f}  TN={tn} FP={fp} FN={fn} TP={tp}\")\n",
    "\n",
    "# Save confusion matrix & PR curve\n",
    "FIG_DIR = OUT_ROOT / \"figs\"\n",
    "cm = np.array([[best_cm[0], best_cm[1]],[best_cm[2], best_cm[3]]], dtype=int)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(cm, cmap=\"Blues\")\n",
    "plt.xticks([0,1], [\"No-Ship\",\"Ship\"]); plt.yticks([0,1], [\"No-Ship\",\"Ship\"])\n",
    "for (i,j),v in np.ndenumerate(cm): plt.text(j,i,str(v),ha=\"center\",va=\"center\")\n",
    "plt.title(f\"Confusion Matrix (val @ F1={best_f1:.3f}, thr={best_t:.2f})\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.tight_layout()\n",
    "plt.savefig(FIG_DIR/\"vit_confusion.png\", dpi=200); plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(rec, prec)\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "plt.title(f\"Precision–Recall (AP={ap:.3f})\"); plt.grid(True); plt.tight_layout()\n",
    "plt.savefig(FIG_DIR/\"vit_pr.png\", dpi=200); plt.close()\n",
    "\n",
    "with open(OUT_ROOT/\"vit_val_metrics.json\",\"w\") as f:\n",
    "    json.dump({\n",
    "        \"AP\": float(ap), \"F1_best\": float(best_f1), \"thr_best\": float(best_t),\n",
    "        \"cm_best\": {\"tn\": int(best_cm[0]), \"fp\": int(best_cm[1]),\n",
    "                    \"fn\": int(best_cm[2]), \"tp\": int(best_cm[3])},\n",
    "        \"img_size\": int(IMG_SIZE),\n",
    "        \"vit\": {\"patch\": 16, \"dim\": 256, \"depth\": 6, \"heads\": 4, \"mlp_dim\": 512, \"drop\": 0.1}\n",
    "    }, f, indent=2)\n",
    "\n",
    "np.save(OUT_ROOT/\"vit_val_probs.npy\",  probs)\n",
    "np.save(OUT_ROOT/\"vit_val_labels.npy\", y_true)\n",
    "\n",
    "print(\"Saved → figs: vit_confusion.png, vit_pr.png | metrics: vit_val_metrics.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af22b115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split counts:\n",
      "split2\n",
      "train    2799\n",
      "val       733\n",
      "test      468\n",
      "Name: count, dtype: int64\n",
      "label   No-Ship  Ship\n",
      "split2               \n",
      "test        336   132\n",
      "train      2111   688\n",
      "val         553   180\n",
      "[TEST] AP=0.999 | Best-F1=0.985 @ thr=0.21\n",
      "[TEST] CM @ best-F1 [TN FP FN TP]: (332, 4, 0, 132)\n",
      "[TEST] t=0.10  P=0.943  R=1.000  F1=0.971  TN=328 FP=8 FN=0 TP=132\n",
      "[TEST] t=0.15  P=0.964  R=1.000  F1=0.981  TN=331 FP=5 FN=0 TP=132\n",
      "[TEST] t=0.20  P=0.964  R=1.000  F1=0.981  TN=331 FP=5 FN=0 TP=132\n",
      "[TEST] t=0.25  P=0.970  R=0.992  F1=0.981  TN=332 FP=4 FN=1 TP=131\n",
      "[TEST] t=0.30  P=0.970  R=0.992  F1=0.981  TN=332 FP=4 FN=1 TP=131\n",
      "[TEST] t=0.40  P=0.970  R=0.985  F1=0.977  TN=332 FP=4 FN=2 TP=130\n",
      "[TEST] t=0.50  P=0.970  R=0.977  F1=0.974  TN=332 FP=4 FN=3 TP=129\n",
      "Saved TEST → figs: vit_test_confusion.png, vit_test_pr.png | metrics: vit_test_metrics.json\n",
      "Test scenes stored in: outputs\\test_scenes.txt\n"
     ]
    }
   ],
   "source": [
    "# FINAL TEST EVALUATION (scene-level, ViT)\n",
    "import os, json, re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, average_precision_score\n",
    "\n",
    "IMG_SIZE     = 128         \n",
    "PATCH        = 16\n",
    "DIM          = 256\n",
    "DEPTH        = 6\n",
    "HEADS        = 4\n",
    "MLP_DIM      = 512\n",
    "DROP         = 0.10\n",
    "L2_REG       = 1e-6\n",
    "WEIGHTS_PATH = Path(\"outputs/models/vit_final.weights.h5\")\n",
    "OUT_ROOT     = Path(\"outputs\"); OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR      = OUT_ROOT / \"figs\"; FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TEST_SCENES_TXT = OUT_ROOT / \"test_scenes.txt\"  \n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "for g in tf.config.list_physical_devices(\"GPU\"):\n",
    "    try: tf.config.experimental.set_memory_growth(g, True)\n",
    "    except Exception: pass\n",
    "\n",
    "assert \"df\" in globals(), \"`df` is missing. Run your parsing & split cells first.\"\n",
    "assert {\"path\",\"label\",\"scene\"}.issubset(df.columns), \"df must have: path, label, scene\"\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "if TEST_SCENES_TXT.exists():\n",
    "    test_scenes = set(TEST_SCENES_TXT.read_text().splitlines())\n",
    "else:\n",
    "    all_scenes = sorted(df[\"scene\"].unique())\n",
    "    rng.shuffle(all_scenes)\n",
    "    n_test = max(1, int(len(all_scenes) * 0.10))\n",
    "    test_scenes = set(all_scenes[:n_test])\n",
    "    TEST_SCENES_TXT.write_text(\"\\n\".join(sorted(test_scenes)))\n",
    "\n",
    "df = df.copy()\n",
    "df[\"split2\"] = np.where(df[\"scene\"].isin(test_scenes), \"test\", df.get(\"split\", \"train\"))\n",
    "\n",
    "print(\"Split counts:\")\n",
    "print(df[\"split2\"].value_counts())\n",
    "print(df.groupby([\"split2\", df[\"label\"].map({0:\"No-Ship\",1:\"Ship\"})]).size().unstack(fill_value=0))\n",
    "\n",
    "\n",
    "# Build test dataset\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def _decode_resize_fallback(path, label):\n",
    "    img = tf.io.decode_png(tf.io.read_file(path), channels=3)\n",
    "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE), antialias=True)\n",
    "    img = tf.cast(img, tf.float32) / 255.0           \n",
    "    return img, tf.cast(label, tf.int32)\n",
    "\n",
    "decode_fn = globals().get(\"decode_resize\", _decode_resize_fallback)\n",
    "\n",
    "test_df = df[df[\"split2\"] == \"test\"].reset_index(drop=True)\n",
    "assert len(test_df) > 0, \"Empty test set — adjust your split size.\"\n",
    "\n",
    "test_ds = (tf.data.Dataset\n",
    "           .from_tensor_slices((test_df[\"path\"].values, test_df[\"label\"].values))\n",
    "           .map(lambda p,l: decode_fn(p,l), num_parallel_calls=AUTOTUNE)\n",
    "           .batch(64)\n",
    "           .prefetch(AUTOTUNE))\n",
    "\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "def _build_vit_fallback(img_size=IMG_SIZE, patch=PATCH, dim=DIM, depth=DEPTH, heads=HEADS,\n",
    "                        mlp_dim=MLP_DIM, drop=DROP, l2=L2_REG):\n",
    "    assert img_size % patch == 0\n",
    "    n_h = img_size // patch\n",
    "    n_w = img_size // patch\n",
    "    num_patches = n_h * n_w\n",
    "\n",
    "    class AddClassTokenAndPos(layers.Layer):\n",
    "        def __init__(self, num_patches, dim, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.num_patches = num_patches\n",
    "            self.dim = dim\n",
    "            self.cls_token = self.add_weight(\"cls_token\", shape=(1,1,dim),\n",
    "                                             initializer=\"zeros\", trainable=True)\n",
    "            self.pos_emb   = self.add_weight(\"pos_emb\",   shape=(1,num_patches+1,dim),\n",
    "                                             initializer=\"zeros\", trainable=True)\n",
    "        def call(self, tokens):\n",
    "            B = tf.shape(tokens)[0]\n",
    "            cls = tf.repeat(self.cls_token, repeats=B, axis=0)\n",
    "            x = tf.concat([cls, tokens], axis=1)\n",
    "            return x + self.pos_emb\n",
    "\n",
    "    inputs = layers.Input((img_size, img_size, 3))\n",
    "    x = layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(inputs)\n",
    "    x = layers.Conv2D(dim, kernel_size=patch, strides=patch, padding=\"valid\")(x)  \n",
    "    x = layers.Reshape((num_patches, dim))(x)\n",
    "    x = AddClassTokenAndPos(num_patches=num_patches, dim=dim)(x)\n",
    "\n",
    "    for _ in range(depth):\n",
    "        h = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        h = layers.MultiHeadAttention(num_heads=heads, key_dim=dim, dropout=drop)(h, h)\n",
    "        x = layers.Add()([x, layers.Dropout(drop)(h)])\n",
    "        h = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        h = layers.Dense(mlp_dim, activation=\"gelu\")(h)\n",
    "        h = layers.Dropout(drop)(h)\n",
    "        h = layers.Dense(dim)(h)\n",
    "        x = layers.Add()([x, layers.Dropout(drop)(h)])\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    cls = x[:, 0]\n",
    "    cls = layers.Dropout(drop)(cls)\n",
    "    logits = layers.Dense(1,\n",
    "                          kernel_regularizer=regularizers.l2(l2),\n",
    "                          bias_regularizer=regularizers.l2(l2))(cls)  \n",
    "    return models.Model(inputs, logits, name=\"vit_binary\")\n",
    "\n",
    "build_vit = globals().get(\"build_vit\", _build_vit_fallback)\n",
    "\n",
    "assert WEIGHTS_PATH.exists(), f\"Missing weights: {WEIGHTS_PATH}\"\n",
    "model = build_vit(IMG_SIZE, PATCH, DIM, DEPTH, HEADS, MLP_DIM, DROP, L2_REG)\n",
    "model.load_weights(WEIGHTS_PATH)\n",
    "\n",
    "\n",
    "# Predict on TEST and score\n",
    "\n",
    "ys, yh = [], []\n",
    "for x, y in test_ds:\n",
    "    p = model.predict(x, verbose=0)   \n",
    "    ys.append(y.numpy()); yh.append(p)\n",
    "\n",
    "y_true = np.concatenate(ys).astype(int)\n",
    "logits = np.concatenate(yh).ravel().astype(np.float32)\n",
    "probs  = 1.0 / (1.0 + np.exp(-logits))\n",
    "probs  = np.clip(probs, 1e-7, 1-1e-7)\n",
    "\n",
    "ap = average_precision_score(y_true, probs)\n",
    "prec, rec, thr = precision_recall_curve(y_true, probs)\n",
    "\n",
    "ts = np.linspace(0.05, 0.50, 46)  \n",
    "best_f1, best_t, best_cm = -1.0, 0.5, None\n",
    "for t in ts:\n",
    "    yhat = (probs >= t).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, yhat, labels=[0,1]).ravel()\n",
    "    P = tp/(tp+fp+1e-9); R = tp/(tp+fn+1e-9)\n",
    "    F1 = 2*P*R/(P+R+1e-9)\n",
    "    if F1 > best_f1:\n",
    "        best_f1, best_t, best_cm = F1, t, (tn, fp, fn, tp)\n",
    "\n",
    "print(f\"[TEST] AP={ap:.3f} | Best-F1={best_f1:.3f} @ thr={best_t:.2f}\")\n",
    "print(\"[TEST] CM @ best-F1 [TN FP FN TP]:\", best_cm)\n",
    "for t in [0.10, 0.15, 0.20, 0.25, 0.30, 0.40, 0.50]:\n",
    "    yhat = (probs >= t).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, yhat, labels=[0,1]).ravel()\n",
    "    P = tp/(tp+fp+1e-9); R = tp/(tp+fn+1e-9); F1 = 2*P*R/(P+R+1e-9)\n",
    "    print(f\"[TEST] t={t:.2f}  P={P:.3f}  R={R:.3f}  F1={F1:.3f}  TN={tn} FP={fp} FN={fn} TP={tp}\")\n",
    "\n",
    "\n",
    "# Save TEST figs & metrics\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = np.array([[best_cm[0], best_cm[1]],\n",
    "               [best_cm[2], best_cm[3]]], dtype=int)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(cm, cmap=\"Blues\")\n",
    "plt.xticks([0,1], [\"No-Ship\",\"Ship\"]); plt.yticks([0,1], [\"No-Ship\",\"Ship\"])\n",
    "for (i,j),v in np.ndenumerate(cm): plt.text(j,i,str(v),ha=\"center\",va=\"center\")\n",
    "plt.title(f\"Confusion Matrix (TEST @ F1={best_f1:.3f}, thr={best_t:.2f})\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.tight_layout()\n",
    "plt.savefig(FIG_DIR/\"vit_test_confusion.png\", dpi=200); plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(rec, prec)\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "plt.title(f\"Precision–Recall (TEST AP={ap:.3f})\")\n",
    "plt.grid(True); plt.tight_layout()\n",
    "plt.savefig(FIG_DIR/\"vit_test_pr.png\", dpi=200); plt.close()\n",
    "\n",
    "with open(OUT_ROOT/\"vit_test_metrics.json\",\"w\") as f:\n",
    "    json.dump({\n",
    "        \"AP\": float(ap), \"F1_best\": float(best_f1), \"thr_best\": float(best_t),\n",
    "        \"cm_best\": {\"tn\": int(best_cm[0]), \"fp\": int(best_cm[1]),\n",
    "                    \"fn\": int(best_cm[2]), \"tp\": int(best_cm[3])},\n",
    "        \"img_size\": int(IMG_SIZE),\n",
    "        \"vit\": {\"patch\": PATCH, \"dim\": DIM, \"depth\": DEPTH, \"heads\": HEADS,\n",
    "                \"mlp_dim\": MLP_DIM, \"drop\": DROP, \"l2\": L2_REG},\n",
    "        \"test_scenes_file\": str(TEST_SCENES_TXT)\n",
    "    }, f, indent=2)\n",
    "\n",
    "np.save(OUT_ROOT/\"vit_test_probs.npy\",  probs)\n",
    "np.save(OUT_ROOT/\"vit_test_labels.npy\", y_true)\n",
    "\n",
    "print(\"Saved TEST → figs: vit_test_confusion.png, vit_test_pr.png | metrics: vit_test_metrics.json\")\n",
    "print(\"Test scenes stored in:\", TEST_SCENES_TXT)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
